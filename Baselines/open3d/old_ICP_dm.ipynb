{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-23T09:49:14.366Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os, shutil\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import open3d\n",
    "import open3d as o3d\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import glob\n",
    "from torch.utils.data import Dataset\n",
    "from open3d import read_point_cloud\n",
    "from tqdm import *\n",
    "\n",
    "# import argparse\n",
    "# import functools\n",
    "# print = functools.partial(print,flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-23T09:49:14.371Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set constants and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-23T09:49:14.375Z"
    }
   },
   "outputs": [],
   "source": [
    "FIRST_PCD = 0\n",
    "FINAL_PCD = 699\n",
    "pcd_path = \"./bench_dataset/ref_run_ccw_r_tenth_scan/\"\n",
    "\n",
    "# FIRST_PCD = 0\n",
    "# FINAL_PCD = 699\n",
    "# pcd_path = \"./bench_dataset/ref_run_ccw_r/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-23T09:49:14.377Z"
    }
   },
   "outputs": [],
   "source": [
    "# APPLY_PREPROCESS = True\n",
    "# VOXEL_SZ = 0.4\n",
    "\n",
    "# FIRST_PCD = 0\n",
    "# FINAL_PCD = 699\n",
    "# pcd_path = \"/home/sabyasachi/Projects/ati/data/data/datasets/IISC/2019-02-26/ref_run_ccw_r/complete/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-23T09:49:14.379Z"
    }
   },
   "outputs": [],
   "source": [
    "EVERY_NTH_COUNT = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### My I/O functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-23T09:49:14.383Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def filter_pcd(old_pcd,\n",
    "               apply_downsample = True,\n",
    "               downsample_voxel_size = VOXEL_SZ,\n",
    "               \n",
    "               apply_outlier_removal = True,\n",
    "               downsample_radius = 1,\n",
    "               downsample_neighbors = 20,\n",
    "               \n",
    "               apply_crop = True,\n",
    "               crop_min_arr = np.array([-100,-100,0]),\n",
    "               crop_max_arr = np.array([100,100,100]),\n",
    "               \n",
    "               apply_cluster = False,\n",
    "               cluster_neighbours = 30,\n",
    "               cluster_labels = 2):\n",
    "    np.random.seed(0)\n",
    "    pcd = copy.deepcopy(old_pcd)\n",
    "    \n",
    "    if apply_outlier_removal:\n",
    "        denser_pcd, ind = o3d.geometry.radius_outlier_removal(pcd,\n",
    "                                                              nb_points = downsample_neighbors,\n",
    "                                                              radius    = downsample_radius)\n",
    "        pcd = denser_pcd\n",
    "    \n",
    "    if apply_downsample:\n",
    "        voxel_down_pcd = o3d.geometry.voxel_down_sample(pcd, voxel_size = downsample_voxel_size)\n",
    "        pcd = voxel_down_pcd\n",
    "    \n",
    "    if apply_crop:\n",
    "        cropped_pcd = o3d.geometry.crop_point_cloud(pcd, crop_min_arr, crop_max_arr)\n",
    "        pcd = cropped_pcd\n",
    "\n",
    "    if apply_cluster:\n",
    "        few_pts = np.asarray(pcd.points)\n",
    "        try:\n",
    "            few_pts_reduced = LocallyLinearEmbedding(n_neighbors=cluster_neighbours, n_components=2).fit_transform(few_pts)\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                few_pts_reduced = LocallyLinearEmbedding(n_neighbors=cluster_neighbours, n_components=2, eigen_solver='dense').fit_transform(few_pts)\n",
    "            except Exception as e:\n",
    "                few_pts_reduced = few_pts\n",
    "        clf = MeanShift().fit(few_pts_reduced)\n",
    "        pcd.points = o3d.utility.Vector3dVector(few_pts[clf.labels_ < cluster_labels])\n",
    "    \n",
    "    return pcd\n",
    "\n",
    "def make_2d(pcd):\n",
    "    new_pcd = copy.deepcopy(pcd)\n",
    "    new_pts = np.concatenate([np.asarray(pcd.points)[:,:-1],np.zeros((len(pcd.points),1))], axis=1)\n",
    "    new_pcd.points = o3d.utility.Vector3dVector(new_pts)\n",
    "    return new_pcd\n",
    "\n",
    "def draw_pcd(pcd, where='mat_2d'):    \n",
    "    if where is 'opn_nb':\n",
    "        visualizer = o3d.JVisualizer()\n",
    "        visualizer.add_geometry(pcd)\n",
    "        visualizer.show()\n",
    "    elif where is 'opn_view':\n",
    "        o3d.visualization.draw_geometries([pcd], width=1280, height=800)\n",
    "    elif where is 'mat_3d':\n",
    "        plt.figure()\n",
    "        pts = np.asarray(pcd.points)\n",
    "        plt.scatter(pts[:,0], pts[:,1], pts[:,2])\n",
    "        plt.show()\n",
    "    elif where is 'mat_2d':\n",
    "        plt.figure()\n",
    "        pts = np.asarray(pcd.points)\n",
    "        plt.scatter(pts[:,0], pts[:,1], s=1)\n",
    "        plt.show()\n",
    "        \n",
    "def preprocess_pcd(old_pcd):\n",
    "    some_pcd = copy.deepcopy(old_pcd)\n",
    "    \n",
    "    # Remove zero points\n",
    "    some_arr = np.asarray(some_pcd.points)\n",
    "    some_arr = np.array([(x,y,z) for x, y, z in some_arr if not (x == 0 and y == 0 and z == 0)])\n",
    "    some_pcd.points = o3d.utility.Vector3dVector(some_arr)\n",
    "    \n",
    "    # Remove outliers and crop ground circle points\n",
    "    some_pcd = filter_pcd(some_pcd,\n",
    "                          apply_downsample = False,\n",
    "                          apply_outlier_removal = True,\n",
    "                          apply_crop = True,\n",
    "                          apply_cluster = False)\n",
    "    \n",
    "    # Make it 2d\n",
    "    some_pcd = make_2d(some_pcd)\n",
    "    \n",
    "    # Downsample\n",
    "    for _ in range(3):\n",
    "        some_pcd = filter_pcd(some_pcd,\n",
    "                              apply_downsample = True,\n",
    "                              apply_outlier_removal = False,\n",
    "                              apply_crop = False,\n",
    "                              apply_cluster = False)\n",
    "    return some_pcd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICP functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-23T09:49:14.387Z"
    }
   },
   "outputs": [],
   "source": [
    "# import utils\n",
    "# from dataset_loader import SimulatedPointCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-23T09:49:14.389Z"
    }
   },
   "outputs": [],
   "source": [
    "def transform_to_global_2D(pose, obs_local):\n",
    "\"\"\" \n",
    "transform local point cloud to global frame\n",
    "row-based matrix product\n",
    "pose: <Bx3> each row represents <x,y,theta>\n",
    "obs_local: <BxLx2> \n",
    "\"\"\"\n",
    "L = obs_local.shape[1]\n",
    "# c0 is the loc of sensor in global coord. frame c0: <Bx2>\n",
    "c0, theta0 = pose[:, 0:2], pose[:, 2]\n",
    "c0 = c0.unsqueeze(1).expand(-1, L, -1)  # <BxLx2>\n",
    "\n",
    "cos = torch.cos(theta0).unsqueeze(-1).unsqueeze(-1)\n",
    "sin = torch.sin(theta0).unsqueeze(-1).unsqueeze(-1)\n",
    "R_transpose = torch.cat((cos, sin, -sin, cos), dim=1).reshape(-1, 2, 2)\n",
    "\n",
    "obs_global = torch.bmm(obs_local, R_transpose) + c0\n",
    "return obs_global\n",
    "\n",
    "def transform_to_global_AVD(pose, obs_local):\n",
    "\"\"\"\n",
    "transform obs local coordinate to global corrdinate frame\n",
    ":param pose: <Bx3> <x,z,theta> y = 0\n",
    ":param obs_local: <BxLx3> (unorganized) or <BxHxWx3> (organized)\n",
    ":return obs_global: <BxLx3> (unorganized) or <BxHxWx3> (organized)\n",
    "\"\"\"\n",
    "is_organized = 1 if len(obs_local.shape) == 4 else 0\n",
    "b = obs_local.shape[0]\n",
    "if is_organized:\n",
    "    H,W = obs_local.shape[1:3]\n",
    "    obs_local = obs_local.view(b,-1,3) # <BxLx3>\n",
    "\n",
    "L = obs_local.shape[1]\n",
    "\n",
    "c0, theta0 = pose[:,0:2],pose[:,2] # c0 is the loc of sensor in global coord frame c0 <Bx2> <x,z>\n",
    "\n",
    "zero = torch.zeros_like(c0[:,:1])\n",
    "c0 = torch.cat((c0,zero),-1) # <Bx3> <x,z,y=0>\n",
    "c0 = c0[:,[0,2,1]] # <Bx3> <x,y=0,z>\n",
    "c0 = c0.unsqueeze(1).expand(-1,L,-1) # <BxLx3>\n",
    "\n",
    "cos = torch.cos(theta0).unsqueeze(-1).unsqueeze(-1)\n",
    "sin = torch.sin(theta0).unsqueeze(-1).unsqueeze(-1)\n",
    "zero = torch.zeros_like(sin)\n",
    "one = torch.ones_like(sin)\n",
    "\n",
    "R_y_transpose = torch.cat((cos,zero,-sin,zero,one,zero,sin,zero,cos),dim=1).reshape(-1,3,3)\n",
    "obs_global = torch.bmm(obs_local,R_y_transpose) + c0\n",
    "if is_organized:\n",
    "    obs_global = obs_global.view(b,H,W,3)\n",
    "return obs_global\n",
    "\n",
    "\n",
    "def rigid_transform_kD(A, B):\n",
    "\"\"\"\n",
    "Find optimal transformation between two sets of corresponding points\n",
    "Adapted from: http://nghiaho.com/uploads/code/rigid_transform_3D.py_\n",
    "Args:\n",
    "    A.B: <Nxk> each row represent a k-D points\n",
    "Returns:\n",
    "    R: kxk\n",
    "    t: kx1\n",
    "    B = R*A+t\n",
    "\"\"\"\n",
    "assert len(A) == len(B)\n",
    "N,k = A.shape\n",
    "\n",
    "centroid_A = np.mean(A, axis=0)\n",
    "centroid_B = np.mean(B, axis=0)\n",
    "\n",
    "# centre the points\n",
    "AA = A - np.tile(centroid_A, (N, 1))\n",
    "BB = B - np.tile(centroid_B, (N, 1))\n",
    "\n",
    "H = np.matmul(np.transpose(AA) , BB)\n",
    "U, S, Vt = np.linalg.svd(H)\n",
    "R = np.matmul(Vt.T , U.T)\n",
    "\n",
    "# special reflection case\n",
    "if np.linalg.det(R) < 0:\n",
    "    Vt[k-1,:] *= -1\n",
    "    R = np.matmul(Vt.T , U.T)\n",
    "\n",
    "t = np.matmul(-R,centroid_A.T) + centroid_B.T\n",
    "t = np.expand_dims(t,-1)\n",
    "return R, t\n",
    "\n",
    "def estimate_normal_eig(data):\n",
    "\"\"\"\n",
    "Computes the vector normal to the k-dimensional sample points\n",
    "\"\"\"\n",
    "data -= np.mean(data,axis=0)\n",
    "data = data.T\n",
    "A = np.cov(data)\n",
    "w,v = np.linalg.eig(A)\n",
    "idx = np.argmin(w)\n",
    "v = v[:,idx]\n",
    "v /= np.linalg.norm(v,2)\n",
    "return v\n",
    "\n",
    "def surface_normal(pc,n_neighbors=6):\n",
    "\"\"\"\n",
    "Estimate point cloud surface normal\n",
    "Args:\n",
    "    pc: Nxk matrix representing k-dimensional point cloud\n",
    "\"\"\"\n",
    "\n",
    "n_points,k = pc.shape\n",
    "v = np.zeros_like(pc)\n",
    "\n",
    "# nn search\n",
    "nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='auto').fit(pc)\n",
    "_, indices = nbrs.kneighbors(pc)\n",
    "neighbor_points = pc[indices]\n",
    "for i in range(n_points):\n",
    "    # estimate surface normal\n",
    "    v_tmp = estimate_normal_eig(neighbor_points[i,])\n",
    "    v_tmp[abs(v_tmp)<1e-5] = 0\n",
    "    if v_tmp[0] < 0:\n",
    "        v_tmp *= -1\n",
    "    v[i,:] = v_tmp\n",
    "return v\n",
    "\n",
    "\n",
    "def point2plane_metrics_2D(p,q,v):\n",
    "\"\"\"\n",
    "Point-to-plane minimization\n",
    "Chen, Y. and G. Medioni. “Object Modelling by Registration of Multiple Range Images.” \n",
    "Image Vision Computing. Butterworth-Heinemann . Vol. 10, Issue 3, April 1992, pp. 145-155.\n",
    "\n",
    "Args:\n",
    "    p: Nx2 matrix, moving point locations\n",
    "    q: Nx2 matrix, fixed point locations\n",
    "    v:Nx2 matrix, fixed point normal\n",
    "Returns:\n",
    "    R: 2x2 matrix\n",
    "    t: 2x1 matrix\n",
    "\"\"\"\n",
    "assert q.shape[1] == p.shape[1] == v.shape[1] == 2, 'points must be 2D'\n",
    "\n",
    "p,q,v = np.array(p),np.array(q),np.array(v)\n",
    "c = np.expand_dims(np.cross(p,v),-1)\n",
    "cn = np.concatenate((c,v),axis=1)  # [ci,nix,niy]\n",
    "C = np.matmul(cn.T,cn)\n",
    "if np.linalg.cond(C)>=1/sys.float_info.epsilon:\n",
    "    # handle singular matrix\n",
    "    raise ArithmeticError('Singular matrix')\n",
    "\n",
    "#     print(C.shape)\n",
    "qp = q-p\n",
    "b = np.array([\n",
    "    [(qp*cn[:,0:1]*v).sum()],\n",
    "    [(qp*cn[:,1:2]*v).sum()],\n",
    "    [(qp*cn[:,2:]*v).sum()],\n",
    "])\n",
    "\n",
    "X = np.linalg.solve(C, b)\n",
    "cos_ = np.cos(X[0])[0]\n",
    "sin_ = np.sin(X[0])[0]\n",
    "R = np.array([\n",
    "    [cos_,-sin_],\n",
    "    [sin_,cos_]\n",
    "])\n",
    "t = np.array(X[1:])\n",
    "return R,t\n",
    "\n",
    "def icp(src,dst,nv=None,n_iter=100,init_pose=[0,0,0],torlerance=1e-6,metrics='point',verbose=False):\n",
    "'''\n",
    "Currently only works for 2D case\n",
    "Args:\n",
    "    src: <Nx2> 2-dim moving points\n",
    "    dst: <Nx2> 2-dim fixed points\n",
    "    n_iter: a positive integer to specify the maxium nuber of iterations\n",
    "    init_pose: [tx,ty,theta] initial transformation\n",
    "    torlerance: the tolerance of registration error\n",
    "    metrics: 'point' or 'plane'\n",
    "\n",
    "Return:\n",
    "    src: transformed src points\n",
    "    R: rotation matrix\n",
    "    t: translation vector\n",
    "    R*src + t\n",
    "'''\n",
    "n_src = src.shape[0]\n",
    "if metrics == 'plane' and nv is None:\n",
    "    nv = surface_normal(dst)\n",
    "\n",
    "#src = np.matrix(src)\n",
    "#dst = np.matrix(dst)\n",
    "#Initialise with the initial pose estimation\n",
    "R_init = np.array([[np.cos(init_pose[2]),-np.sin(init_pose[2])],\n",
    "               [np.sin(init_pose[2]), np.cos(init_pose[2])] \n",
    "                  ])\n",
    "t_init = np.array([[init_pose[0]],\n",
    "               [init_pose[1]]\n",
    "                  ])  \n",
    "\n",
    "#src =  R_init*src.T + t_init\n",
    "src = np.matmul(R_init,src.T) + t_init\n",
    "src = src.T\n",
    "\n",
    "R,t = R_init,t_init\n",
    "\n",
    "prev_err = np.inf\n",
    "nbrs = NearestNeighbors(n_neighbors=1, algorithm='auto').fit(dst)\n",
    "for i in range(n_iter):\n",
    "    # Find the nearest neighbours\n",
    "    _, indices = nbrs.kneighbors(src)\n",
    "\n",
    "    # Compute the transformation\n",
    "    if metrics == 'point':\n",
    "        R0,t0 = rigid_transform_kD(src,dst[indices[:,0]])\n",
    "    elif metrics=='plane':\n",
    "        try:\n",
    "            R0,t0 = point2plane_metrics_2D(src,dst[indices[:,0]], nv[indices[:,0]]) \n",
    "        except ArithmeticError:\n",
    "            print('Singular matrix')\n",
    "            return src,R,t\n",
    "    else:\n",
    "        raise ValueError('metrics: {} not recognized.'.format(metrics))\n",
    "    # Update dst and compute error\n",
    "    src = np.matmul(R0,src.T) + t0\n",
    "    src = src.T\n",
    "\n",
    "    R = np.matmul(R0,R)\n",
    "    t = np.matmul(R0,t) + t0\n",
    "    #R = R0*R\n",
    "    #t = R0*t + t0\n",
    "    current_err = np.sqrt((np.array(src-dst[indices[:,0]])**2).sum()/n_src)\n",
    "\n",
    "    if verbose:\n",
    "        print('iter: {}, error: {}'.format(i,current_err))\n",
    "\n",
    "    if  np.abs(current_err - prev_err) < torlerance:\n",
    "        break\n",
    "    else:\n",
    "        prev_err = current_err\n",
    "\n",
    "return src,R,t\n",
    "\n",
    "\n",
    "def compute_ate(output,target):\n",
    "\"\"\"\n",
    "compute absolute trajectory error for avd dataset\n",
    "Args:\n",
    "    output: <Nx3> predicted trajectory positions, where N is #scans\n",
    "    target: <Nx3> ground truth trajectory positions\n",
    "Returns:\n",
    "    trans_error: <N> absolute trajectory error for each pose\n",
    "    output_aligned: <Nx3> aligned position in ground truth coord\n",
    "\"\"\"\n",
    "R,t = rigid_transform_kD(output,target)\n",
    "output_aligned = np.matmul(R , output.T) + t\n",
    "output_aligned = output_aligned.T\n",
    "FIRST_PCD = 0\n",
    "\n",
    "align_error = np.array(output_aligned - target)\n",
    "trans_error = np.sqrt(np.sum(align_error**2,1))\n",
    "\n",
    "ate = np.sqrt(np.dot(trans_error,trans_error) / len(trans_error))\n",
    "\n",
    "return ate,output_aligned\n",
    "\n",
    "def remove_invalid_pcd(pcd):\n",
    "\"\"\"\n",
    "remove invalid in valid points that have all-zero coordinates\n",
    "pcd: open3d pcd objective\n",
    "\"\"\"\n",
    "pcd_np = np.asarray(pcd.points) # <Nx3>\n",
    "non_zero_coord = np.abs(pcd_np) > 1e-6 # <Nx3>\n",
    "valid_ind = np.sum(non_zero_coord,axis=-1)>0 #<N>\n",
    "valid_ind = list(np.nonzero(valid_ind)[0])\n",
    "valid_pcd = open3d.select_down_sample(pcd,valid_ind)\n",
    "return valid_pcd\n",
    "\n",
    "def ang2mat(theta):\n",
    "c = np.cos(theta)\n",
    "s = np.sin(theta)\n",
    "R = np.array([[c,-s],[s,c]])\n",
    "return R\n",
    "\n",
    "def cat_pose_2D(pose0,pose1):\n",
    "\"\"\"\n",
    "pose0, pose1: <Nx3>, numpy array\n",
    "each row: <x,y,theta>\n",
    "\"\"\"\n",
    "assert(pose0.shape==pose1.shape)\n",
    "n_pose = pose0.shape[0]\n",
    "pose_out = np.zeros_like(pose0) \n",
    "for i in range(n_pose):\n",
    "    R0 = ang2mat(pose0[i,-1])\n",
    "    R1 = ang2mat(pose1[i,-1])\n",
    "    t0 = np.expand_dims(pose0[i,:2],-1)\n",
    "    t1 = np.expand_dims(pose1[i,:2],-1)\n",
    "\n",
    "    R = np.matmul(R1,R0)\n",
    "    theta = np.arctan2(R[1,0],R[0,0])\n",
    "    t = np.matmul(R1,t0) + t1\n",
    "    pose_out[i,:2] = t.T\n",
    "    pose_out[i,2] = theta\n",
    "return pose_out\n",
    "\n",
    "def convert_depth_map_to_pc(depth,fxy,cxy,max_depth=7000,depth_scale=2000):\n",
    "\"\"\"\n",
    "create point cloud from depth map and camera instrinsic\n",
    "depth: <hxw> numpy array\n",
    "fxy: [fx,fy]\n",
    "cxy: [cx,cy]\n",
    "\"\"\"\n",
    "fx,fy = fxy \n",
    "cx,cy = cxy\n",
    "h,w = depth.shape\n",
    "\n",
    "c,r = np.meshgrid(range(1,w+1), range(1,h+1))\n",
    "invalid = depth >= max_depth\n",
    "depth[invalid] = 0\n",
    "\n",
    "z = depth / float(depth_scale)\n",
    "x = z * (c-cx) / fx\n",
    "y = z * (r-cy) / fycolor\n",
    "xyz = np.dstack((x,y,z)).astype(np.float32)\n",
    "return xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-23T09:49:14.392Z"
    }
   },
   "outputs": [],
   "source": [
    "def transform_to_global_open3d(pose,local_pcd):\n",
    "    pcd = copy.deepcopy(local_pcd)\n",
    "    n_pcd = len(pcd)\n",
    "    for i in range(n_pcd):\n",
    "        tx,ty,theta = pose[i,:]\n",
    "        cos,sin = np.cos(theta),np.sin(theta)\n",
    "        trans = np.array([\n",
    "                        [cos,-sin,0,tx],\n",
    "                        [sin,cos,0,ty],\n",
    "                        [0,0,1,0],\n",
    "                        [0,0,0,1],\n",
    "                        ])\n",
    "        pcd[i].transform(trans) \n",
    "    return pcd\n",
    "\n",
    "\n",
    "def np_to_pcd(xyz):\n",
    "    \"\"\"\n",
    "    convert numpy array to point cloud object in open3d\n",
    "    \"\"\"\n",
    "    xyz = xyz.reshape(-1,3)\n",
    "    pcd = o3d.PointCloud()\n",
    "    pcd.points = o3d.Vector3dVector(xyz)\n",
    "    pcd.paint_uniform_color(np.random.rand(3,))\n",
    "    return pcd\n",
    "\n",
    "\n",
    "def load_obs_global_est(file_name):\n",
    "    \"\"\"\n",
    "    load saved obs_global_est.npy file and convert to point cloud object\n",
    "    \"\"\"\n",
    "    obs_global_est = np.load(file_name)\n",
    "    n_pc = obs_global_est.shape[0]\n",
    "    pcds = o3d.PointCloud()\n",
    "\n",
    "    for i in range(n_pc):\n",
    "        xyz = obs_global_est[i,:,:]\n",
    "        current_pcd = np_to_pcd(xyz)\n",
    "        pcds += current_pcd\n",
    "    return pcds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-23T09:49:14.394Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_global_point_cloud(point_cloud, pose, valid_points, save_dir, **kwargs):\n",
    "    if torch.is_tensor(point_cloud):\n",
    "        point_cloud = point_cloud.cpu().detach().numpy()\n",
    "    if torch.is_tensor(pose):\n",
    "        pose = pose.cpu().detach().numpy()\n",
    "    if torch.is_tensor(valid_points):\n",
    "        valid_points = valid_points.cpu().detach().numpy()\n",
    "\n",
    "    file_name = 'global_map_pose'\n",
    "    if kwargs is not None:\n",
    "        for k, v in kwargs.items():\n",
    "            file_name = file_name + '_' + str(k) + '_' + str(v)\n",
    "    save_name = os.path.join(save_dir, file_name)\n",
    "\n",
    "    bs = point_cloud.shape[0]\n",
    "    for i in range(bs):\n",
    "        current_pc = point_cloud[i, :, :]\n",
    "        idx = valid_points[i, ] > 0\n",
    "        current_pc = current_pc[idx]\n",
    "\n",
    "        plt.plot(current_pc[:, 0], current_pc[:, 1], '.')\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim(ax.get_ylim()[::-1])\n",
    "    plt.plot(pose[:, 0], pose[:, 1], color='black')\n",
    "#     plt.savefig(save_name)\n",
    "    plt.show()\n",
    "#     plt.close()\n",
    "\n",
    "def save_global_point_cloud_open3d(point_cloud,pose,save_dir):\n",
    "    file_name = 'global_map_pose'\n",
    "    save_name = os.path.join(save_dir, file_name)\n",
    "\n",
    "    n_pcd = len(point_cloud)\n",
    "    for i in range(n_pcd):\n",
    "        current_pc = np.asarray(point_cloud[i].points)\n",
    "        plt.plot(current_pc[:, 0], current_pc[:, 1], '.',markersize=1)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim(ax.get_ylim()[::-1])\n",
    "    plt.plot(pose[:, 0], pose[:, 1], color='black')\n",
    "    plt.savefig(save_name)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-23T09:49:14.397Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_valid_points(local_point_cloud):\n",
    "    \"\"\"\n",
    "    find valid points in local point cloud\n",
    "        invalid points have all zeros local coordinates\n",
    "    local_point_cloud: <BxNxk> \n",
    "    valid_points: <BxN> indices  of valid point (0/1)\n",
    "    \"\"\"\n",
    "    eps = 1e-6\n",
    "    non_zero_coord = torch.abs(local_point_cloud) > eps\n",
    "    valid_points = torch.sum(non_zero_coord, dim=-1)\n",
    "    valid_points = valid_points > 0\n",
    "    return valid_points\n",
    "\n",
    "\n",
    "class SimulatedPointCloud(Dataset):\n",
    "    def __init__(self, root, trans_by_pose=None):\n",
    "        # trans_by_pose: <Bx3> pose\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self._trans_by_pose = trans_by_pose\n",
    "        file_list = glob.glob(os.path.join(self.root, '*pcd'))\n",
    "        self.file_list = sorted(file_list)\n",
    "\n",
    "        if APPLY_PREPROCESS:\n",
    "            print(\"Applying preprocessing\")\n",
    "            temp_dir = \"./preprocessed_pcds\"\n",
    "            if not os.path.exists(temp_dir):\n",
    "                os.makedirs(temp_dir)\n",
    "            else:\n",
    "                shutil.rmtree(temp_dir)\n",
    "                os.makedirs(temp_dir)\n",
    "                \n",
    "            file_idx = 0\n",
    "            for file_idx in tqdm_notebook(np.arange(start=FIRST_PCD, stop=FINAL_PCD, step=EVERY_NTH_COUNT)):\n",
    "                file = self.file_list[file_idx]\n",
    "                pcd = read_point_cloud(file)\n",
    "                pcd = preprocess_pcd(pcd)\n",
    "                single_pcd_fname = str(file_idx) + \".pcd\"\n",
    "                single_pcd_path = os.path.join(temp_dir, single_pcd_fname)\n",
    "                o3d.io.write_point_cloud(single_pcd_path, pcd)\n",
    "                file_idx  += 1\n",
    "            \n",
    "            root = temp_dir\n",
    "            self.root = os.path.expanduser(root)\n",
    "            file_list = glob.glob(os.path.join(self.root, '*pcd'))\n",
    "            self.file_list = sorted(file_list)\n",
    "            \n",
    "        print(\"Reading pcds\")\n",
    "        point_clouds = [] #a list of tensor <Lx2>\n",
    "        for file in tqdm_notebook(self.file_list):\n",
    "            pcd = read_point_cloud(file)\n",
    "            current_point_cloud = np.asarray(pcd.points, dtype=np.float32)[:, 0:2]        \n",
    "            point_clouds.append(current_point_cloud)\n",
    "\n",
    "        point_clouds = np.asarray(point_clouds)\n",
    "        try:\n",
    "            self.point_clouds = torch.from_numpy(point_clouds) # <NxLx2>\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "            # Handling the uniform size change across all point clouds\n",
    "            NEW_SIZE = max([current_point_cloud.shape[0] for current_point_cloud in point_clouds]) + 1\n",
    "            print('Ignore above warning\\nAligning pt clouds to equal size of {}'.format(NEW_SIZE))\n",
    "\n",
    "            np.random.seed(0)\n",
    "            new_point_clouds = []\n",
    "            for current_point_cloud in tqdm_notebook(point_clouds):\n",
    "                old_size = current_point_cloud.shape[0]\n",
    "                delta_size = NEW_SIZE - old_size\n",
    "                idx_list = np.random.randint(low=0, high=old_size, size=delta_size)\n",
    "                delta_point_cloud = np.array([current_point_cloud[idx] for idx in idx_list])\n",
    "                new_point_cloud = np.concatenate([current_point_cloud, delta_point_cloud])\n",
    "                new_point_clouds.append(new_point_cloud)\n",
    "            point_clouds = np.asarray(new_point_clouds)\n",
    "            self.point_clouds = torch.from_numpy(point_clouds) # <NxLx2>\n",
    "\n",
    "        self.valid_points = find_valid_points(self.point_clouds) # <NxL>\n",
    "\n",
    "        # number of points in each point cloud\n",
    "        self.n_obs = self.point_clouds.shape[1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        pcd = self.point_clouds[index,:,:]  # <Lx2>\n",
    "        valid_points = self.valid_points[index,:]\n",
    "        if self._trans_by_pose is not None:\n",
    "            pcd = pcd.unsqueeze(0)  # <1XLx2>\n",
    "            pose = self._trans_by_pose[index, :].unsqueeze(0)  # <1x3>\n",
    "            pcd = utils.transform_to_global_2D(pose, pcd).squeeze(0)\n",
    "        return pcd,valid_points\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.point_clouds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start ICP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-23T09:49:14.404Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = SimulatedPointCloud(pcd_path)\n",
    "n_pc = len(dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pose_est = np.zeros((n_pc,3),dtype=np.float32)\n",
    "\n",
    "def parallel_thread(idx):\n",
    "    dst,valid_dst = dataset[idx] \n",
    "    src,valid_src = dataset[idx+1]\n",
    "    \n",
    "    dst = dst[valid_dst,:].numpy()\n",
    "    src = src[valid_src,:].numpy()\n",
    "\n",
    "    _,R0,t0 = icp(src,dst,metrics='plane')\n",
    "    if idx == 0: \n",
    "        R_cum = R0\n",
    "        t_cum = t0\n",
    "    else:\n",
    "        R_cum = np.matmul(R_cum , R0)\n",
    "        t_cum = np.matmul(R_cum,t0) + t_cum\n",
    "    \n",
    "    pose_est[idx+1,:2] = t_cum.T\n",
    "    pose_est[idx+1,2] = np.arctan2(R_cum[1,0],R_cum[0,0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "file_indices = [idx for idx in tqdm_notebook(range(n_pc-1))]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "process_pool = Pool(cpu_count()-1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('running icp in parallel')\n",
    "__something__ = [each for each in \\\n",
    "                 tqdm_notebook(process_pool.imap_unordered( parallel_thread, file_indices),\n",
    "                               total=len(file_indices))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-23T09:49:14.503Z"
    }
   },
   "outputs": [],
   "source": [
    "pose_est = np.zeros((n_pc,3), dtype=np.float32)\n",
    "print('running icp')\n",
    "\n",
    "for idx in tqdm_notebook(range(n_pc-1)):\n",
    "    dst,valid_dst = dataset[idx] \n",
    "    src,valid_src = dataset[idx+1]\n",
    "    \n",
    "    dst = dst[valid_dst,:].numpy()\n",
    "    src = src[valid_src,:].numpy()\n",
    "\n",
    "    _,R0,t0 = icp(src, dst, metrics='plane')\n",
    "    if idx == 0: \n",
    "        R_cum = R0\n",
    "        t_cum = t0\n",
    "    else:\n",
    "        R_cum = np.matmul(R_cum , R0)\n",
    "        t_cum = np.matmul(R_cum,t0) + t_cum\n",
    "    \n",
    "    pose_est[idx+1,:2] = t_cum.T\n",
    "    pose_est[idx+1,2] = np.arctan2(R_cum[1,0],R_cum[0,0]) \n",
    "\n",
    "# save_name = os.path.join(checkpoint_dir,'pose_est.npy')\n",
    "# np.save(save_name,pose_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-23T09:49:14.506Z"
    }
   },
   "outputs": [],
   "source": [
    "pose_est = torch.from_numpy(pose_est)\n",
    "local_pc,valid_id = dataset[:]\n",
    "global_pc = transform_to_global_2D(pose_est,local_pc)\n",
    "plot_global_point_cloud(global_pc,pose_est,valid_id,\"./\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.plot(dataset[0][0][:,0].numpy(), dataset[0][0][:,1].numpy(), '.')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.plot(dataset[1][0][:,0].numpy(), dataset[1][0][:,1].numpy(), '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-23T09:49:14.544Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(pose_est[:,0].numpy(), pose_est[:,1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

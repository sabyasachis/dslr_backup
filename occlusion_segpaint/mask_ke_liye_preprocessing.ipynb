{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T22:31:04.882099Z",
     "start_time": "2020-02-06T22:31:04.386859Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "def retrieve_elements_from_indices(tensor, indices):\n",
    "    flattened_tensor = tensor.flatten(start_dim=2)\n",
    "    output = flattened_tensor.gather(dim=2, index=indices.flatten(start_dim=2)).view_as(indices)\n",
    "    return output\n",
    "\n",
    "##############\n",
    "count = 0\n",
    "# -------------------------------------------------------------------------\n",
    "# Handy Utilities\n",
    "# -------------------------------------------------------------------------\n",
    "def to_polar_np(velo):\n",
    "    if len(velo.shape) == 4:\n",
    "        velo = velo.transpose(1, 2, 3, 0)\n",
    "\n",
    "    if velo.shape[2] > 4:\n",
    "        assert velo.shape[0] <= 4\n",
    "        velo = velo.transpose(1, 2, 0, 3)\n",
    "        switch=True\n",
    "    else:\n",
    "        switch=False\n",
    "    print(\"inside to velo\")\n",
    "#     print(velo[:,:,3][(velo[:,:,3]!=0)&(velo[:,:,3]!=1)])\n",
    "    \n",
    "    # assumes r x n/r x (3,4) velo\n",
    "    dist = np.sqrt(velo[:, :, 0] ** 2 + velo[:, :, 1] ** 2)\n",
    "    # theta = np.arctan2(velo[:, 1], velo[:, 0])\n",
    "    out = np.stack([dist, velo[:, :, 2],velo[:,:,3]], axis=2)\n",
    "    #out = np.stack([dist, velo[:, :, 2]], axis=2)\n",
    "    \n",
    "    if switch:\n",
    "        out = out.transpose(2, 0, 1, 3)\n",
    "\n",
    "    if len(velo.shape) == 4: \n",
    "        out = out.transpose(3, 0, 1, 2)\n",
    "    \n",
    "    \n",
    "#     print(dist.shape)\n",
    "#     print(\"out ki shape\")\n",
    "#     print(out.shape)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def to_polar(velo):\n",
    "    if len(velo.shape) == 4:\n",
    "        velo = velo.permute(1, 2, 3, 0)\n",
    "\n",
    "    if velo.shape[2] > 4:\n",
    "        assert velo.shape[0] <= 4\n",
    "        velo = velo.permute(1, 2, 0, 3)\n",
    "        switch=True\n",
    "    else:\n",
    "        switch=False\n",
    "    \n",
    "    # assumes r x n/r x (3,4) velo\n",
    "    dist = torch.sqrt(velo[:, :, 0] ** 2 + velo[:, :, 1] ** 2)\n",
    "    # theta = np.arctan2(velo[:, 1], velo[:, 0])\n",
    "    \n",
    "   \n",
    "    \n",
    "    out = torch.stack([dist, velo[:, :, 2]], dim=2)\n",
    "    \n",
    "    if switch:\n",
    "        out = out.permute(2, 0, 1, 3)\n",
    "\n",
    "    if len(velo.shape) == 4: \n",
    "        out = out.permute(3, 0, 1, 2)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def from_polar(velo):\n",
    "    angles = np.linspace(0, np.pi * 2, velo.shape[-1])\n",
    "    dist, z = velo[:, 0], velo[:, 1]\n",
    "    x = torch.Tensor(np.cos(angles)).cuda().unsqueeze(0).unsqueeze(0) * dist\n",
    "    y = torch.Tensor(np.sin(angles)).cuda().unsqueeze(0).unsqueeze(0) * dist\n",
    "    out = torch.stack([x,y,z], dim=1)\n",
    "\n",
    "    return out\n",
    "\n",
    "def from_polar_np(velo):\n",
    "    angles = np.linspace(0, np.pi * 2, velo.shape[-1])\n",
    "    dist, z = velo[:, 0], velo[:, 1]\n",
    "    x = np.cos(angles) * dist\n",
    "    y = np.sin(angles) * dist\n",
    "    out = np.stack([x,y,z], axis=1)\n",
    "    return out.astype('float32')\n",
    "\n",
    "def print_and_log_scalar(writer, name, value, write_no, end_token=''):\n",
    "    if isinstance(value, list):\n",
    "        if len(value) == 0: return \n",
    "        value = torch.mean(torch.stack(value))\n",
    "    zeros = 40 - len(name) \n",
    "    name += ' ' * zeros\n",
    "    print('{} @ write {} = {:.4f}{}'.format(name, write_no, value, end_token))\n",
    "    writer.add_scalar(name, value, write_no)\n",
    "\n",
    "def log_point_clouds(writer, data, name, step):\n",
    "    if len(data.shape) == 3:\n",
    "        data = [data]\n",
    "    \n",
    "    out = np.stack([from_polar(x.transpose(1, 2, 0)) for x in \\\n",
    "            data.cpu().data.numpy()])\n",
    "    out = torch.tensor(out).float()\n",
    "\n",
    "    for i, cloud in enumerate(out):\n",
    "        cloud = cloud.view(-1, 3)\n",
    "        writer.add_embedding(cloud, tag=name + '_%d' % i, global_step=step)\n",
    "\n",
    "def print_and_save_args(args, path):\n",
    "    print(args)\n",
    "    # let's save the args as json to enable easy loading\n",
    "    import json\n",
    "    with open(os.path.join(path, 'args.json'), 'w') as f: \n",
    "        json.dump(vars(args), f)\n",
    "\n",
    "def maybe_create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def remove_zeros(pc):\n",
    "  \n",
    "    \n",
    "    xx = torch.cuda.FloatTensor(pc)\n",
    "#     print(xx.dim())\n",
    "    if xx.dim() == 3: \n",
    "        xx = xx.unsqueeze(0)\n",
    "    \n",
    "    print(\"xx ki shape\")\n",
    "    print(xx.shape)\n",
    "    \n",
    "    \n",
    "#     print(\"xx ki poll khulegi\")\n",
    "#     print(xx[:,2][(xx[:,2]!=0)&(xx[:,2]!=1)])\n",
    "#     print(xx[:,2].unsqueeze(0).shape)\n",
    "   \n",
    "    iters = 0\n",
    "    pad = 2\n",
    "    ks = 5\n",
    "    while (xx[:, 0] == 0).sum() > 0 : \n",
    "        print(iters)\n",
    "        print (\"The number of zeros are:\", (xx[:, 0] == 0).sum())\n",
    "        if iters  > 100:\n",
    "            raise ValueError()\n",
    "            ks += 2\n",
    "            pad += 1\n",
    "#             print(\"iters if ke andar gya\")\n",
    "#         print(\"maa_ka_chodi\")\n",
    "        \n",
    "        mask = (xx[:, 0] == 0).unsqueeze(1).float()\n",
    "        \n",
    "#         print(\"mask is\")\n",
    "#         print(mask)\n",
    "        \n",
    "#         print(\"mask ka shape\")\n",
    "#         print(mask.shape)\n",
    "        \n",
    "        out_a,indices = F.max_pool2d(xx[:, 0], ks, padding=pad, stride=1,return_indices=True)\n",
    "#         print(indices)\n",
    "#         print(indices.shape)\n",
    "#         out_b = -F.max_pool2d(-xx[:, 1], ks, padding=pad, stride=1)\n",
    "        out_b = F.max_pool2d(xx[:, 1], ks, padding=pad, stride=1)\n",
    "        out_c = retrieve_elements_from_indices(xx[:,2].unsqueeze(0), indices)\n",
    "        \n",
    "        print(\"out_b shapee before\")\n",
    "        print(out_b.shape)\n",
    "        \n",
    "        # Changing this since our z = -z from carla\n",
    "        #out_b_ = (xx[:, 1]).min(dim=-1, keepdim=True)[0].expand_as(out_b)\n",
    "        #out_b = torch.cat([out_b_[:, :10], out_b[:, 10:]], dim=1)\n",
    "        out_b = out_b.expand_as(out_a)\n",
    "        \n",
    "        print(\"out_b shapee after\")\n",
    "        print(out_b.shape)\n",
    "        \n",
    "#         print(out_c.shape)\n",
    "      \n",
    "        \n",
    "        out = torch.stack([out_a, out_b,out_c.squeeze(0)], dim=1)\n",
    "#         out = torch.stack([out_a, out_b], dim=1)\n",
    "        \n",
    "        mask = (xx[:, 0] == 0).unsqueeze(1)\n",
    "        mask = mask.float()\n",
    "        \n",
    "        print(\"mask ki shape\")\n",
    "        print(mask.shape)\n",
    "        print(\"out ki shape\")\n",
    "        print(out.shape)\n",
    "        print(\"out_c mein number of ones\")\n",
    "        print(out_c[out_c==1].shape)\n",
    "        \n",
    "        print(\"pehla:\",(xx[:,2][xx[:,2]==1].size()))\n",
    "        xx = xx * (1 - mask) + (mask) * out\n",
    "        \n",
    "        \n",
    "        print(\"doosra:\",(xx[:,2][xx[:,2]==1].size()))\n",
    "        iters += 1\n",
    "    \n",
    "#     print(\"bahar aaj ja aa bhi ab\")\n",
    "    return xx.cpu().data.numpy()\n",
    "\n",
    "# Hello\n",
    "\n",
    "# I read this paper recently and wanted to try my hands on it. Thank you for providing the code for the same.\n",
    "# I have one doubt with regards to the preprocessing step.\n",
    "\n",
    "# The preprocessing step should return a tensor of shape (N, 60, 512, 3) but on trying the code, the tensor comes out to be of shape (N, 40, 256, 3)\n",
    "# Can you clarify whether it's okay to have this output tensor? Also, can you shed some light on the following two lines mentioned in the preprocessing function defined in utils.py\n",
    "\n",
    "# Line 151: dataset = dataset[:, 5:45]\n",
    "# Line 180: return dataset[:, :, :, ::2]\n",
    "# I think that line 151 is reducing the circles or clusters from 60 to 40 and then line 180 is reducing the point per circle/cluster to 256 from 512.\n",
    "\n",
    "# If it's okay to have this as the output tensor, is there any reasoning for reducing clusters and points per cluster?\n",
    "\n",
    "# Please help.\n",
    "\n",
    "# Thanks\n",
    "# Abhishek\n",
    "\n",
    "# @pclucas14\n",
    " \n",
    "# Owner\n",
    "# pclucas14 commented 18 days ago\n",
    "# Hi Abhishek,\n",
    "\n",
    "# What you said is correct. I downsampled the input mainly to reduce the computational burden and improve training speed. I also discarded the outer circles because they usually are the ones that contain the most noise, and thus are the hardest to model. You can bypass these steps if you want, however you may need to make a few changes in the model architecture.\n",
    "\n",
    "# Best,\n",
    "# Lucas\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(dataset, lidar_range):\n",
    "    \n",
    "#     dataset=np.array(dataset,dtype=np.float32)\n",
    "    \n",
    "    dataset[:,:,:,:3]=dataset[:,:,:,:3]/lidar_range   # Max LIDAR value\n",
    "    \n",
    "    dataset = to_polar_np(dataset).transpose(0, 3, 1, 2)\n",
    "    print(\"dataset ki shape\")\n",
    "    print(dataset.shape)\n",
    "    previous = (dataset[:, 0] == 0).sum()\n",
    "\n",
    "    remove = []\n",
    "    for i in range(dataset.shape[0]):\n",
    "#         try:\n",
    "        print(\"the trun going on is :\",i)\n",
    "        pp = remove_zeros(dataset[i]).squeeze(0)\n",
    "#         print(\"pp ki maa ki aankh\")\n",
    "#             print(pp.shape)\n",
    "        dataset[i] = pp\n",
    "#         except:\n",
    "#             remove += [i]\n",
    "#             print(\"exception aa gya ohoho\")\n",
    "\n",
    "    for i in remove:\n",
    "        dataset = np.concatenate([dataset[:i-1], dataset[i+1:]], axis=0)\n",
    "        print(\"yeh_kyun ho rha hain\")\n",
    "    return dataset\n",
    "\n",
    "# def preprocess(dataset, lidar_range):\n",
    "#     # remove outliers \n",
    "#     #min_a, max_a = np.percentile(dataset[:, :, :, [0]], 1), np.percentile(dataset[:, :, :, [0]], 99)\n",
    "#     #min_b, max_b = np.percentile(dataset[:, :, :, [1]], 1), np.percentile(dataset[:, :, :, [1]], 99)\n",
    "#     #min_c, max_c = np.percentile(dataset[:, :, :, [2]], 1), np.percentile(dataset[:, :, :, [2]], 99)\n",
    "#     # min_a, max_a = -41.1245002746582,   36.833248138427734\n",
    "#     # min_b, max_b = -25.833599090576172, 30.474000930786133\n",
    "#     # min_c, max_c = -2.3989999294281006, 0.7383332848548889\n",
    "#     #changes\n",
    "#     min_a, max_a = -71.1245002746582,   66.833248138427734\n",
    "#     min_b, max_b = -55.833599090576172, 60.474000930786133\n",
    "#     min_c, max_c = -30.3989999294281006, 10.7383332848548889\n",
    "\n",
    "\n",
    "#     #changed, commented this, now at phi instead of 40 getting all 60\n",
    "#     #dataset = dataset[:, 5:45]\n",
    "\n",
    "\n",
    "#     mask = np.maximum(dataset[:, :, :, 0] < min_a, dataset[:, :, :, 0] > max_a)\n",
    "#     mask = np.maximum(mask, np.maximum(dataset[:, :, :, 1] < min_b, dataset[:, :, :, 1] > max_b))\n",
    "#     mask = np.maximum(mask, np.maximum(dataset[:, :, :, 2] < min_c, dataset[:, :, :, 2] > max_c))\n",
    "    \n",
    "#     dist = dataset[:, :, :, 0] ** 2 + dataset[:, :, :, 1] ** 2\n",
    "#     mask = np.maximum(mask, dist < 7)\n",
    "\n",
    "#     dataset = dataset * (1 - np.expand_dims(mask, -1))\n",
    "\n",
    "#     #----------------------------------------------------------------------\n",
    "\n",
    "#     #Fix for an erro where np.absolute(dataset).max()   goes to 0 causing failure\n",
    "#     # npmaxError=0.0\n",
    "#     # if np.absolute(dataset).max()==0.0:\n",
    "#     #     npmaxError=1.0\n",
    "\n",
    "#     #----------------------------------------------------------------------\n",
    "\n",
    "#     # dataset /= (np.absolute(dataset).max()+npmaxError)\n",
    "#     dataset /= lidar_range   # Max LIDAR value\n",
    "#     # normalization_factor = (np.absolute(dataset).max()+npmaxError)\n",
    "\n",
    "#     dataset = to_polar_np(dataset).transpose(0, 3, 1, 2)\n",
    "#     previous = (dataset[:, 0] == 0).sum()\n",
    "\n",
    "#     remove = []\n",
    "#     for i in range(dataset.shape[0]):\n",
    "#         #print('processing {}/{}'.format(i, dataset.shape[0]))\n",
    "#         try:\n",
    "#             pp = remove_zeros(dataset[i]).squeeze(0)\n",
    "#             dataset[i] = pp\n",
    "#         except:\n",
    "#             #print('removing %d' % i)\n",
    "#             remove += [i]\n",
    "\n",
    "#     for i in remove:\n",
    "#         dataset = np.concatenate([dataset[:i-1], dataset[i+1:]], axis=0)\n",
    "#     #print(dataset[:, :, :, ::2].shape)\n",
    "\n",
    "\n",
    "#     #changed,  for every 2 entries it retuend 1 entry thereby reducing the 512 to 256\n",
    "#     # return dataset[:, :, :, ::2]\n",
    "#     # print('In preprocess')\n",
    "#     # print(dataset.shape)\n",
    "#     # if give_factor:\n",
    "#     #     return dataset, normalization_factor\n",
    "#     # else:\n",
    "#     return dataset\n",
    "\n",
    "\n",
    "def show_pc(velo, save=0, save_path=None):\n",
    "    import mayavi.mlab\n",
    "\n",
    "    fig = mayavi.mlab.figure(size=(1400, 700), bgcolor=(0,0,0)) \n",
    "\n",
    "    if len(velo.shape) == 3:\n",
    "        if velo.shape[0] == 3 : \n",
    "            velo = velo.transpose(1,2,0)\n",
    "\n",
    "        assert velo.shape[2] == 3\n",
    "        velo = velo.reshape((-1, 3))\n",
    "\n",
    "    max_ = np.absolute(velo[:, :2]).max()\n",
    "    nodes = mayavi.mlab.points3d(\n",
    "        velo[:, 0],   # x\n",
    "        velo[:, 1],   # y\n",
    "        velo[:, 2],   # z\n",
    "        scale_factor=0.008, #0.022,     # scale of the points\n",
    "        figure=fig) \n",
    "    \n",
    "    nodes.glyph.scale_mode = 'scale_by_vector'\n",
    "    color = (velo[:, 2] - velo[:, 2].min()) / (velo[:, 2].max() - velo[:, 2].min())\n",
    "    color = (velo[:, 2] - -0.069667026) / ( 0.0041348818 - -0.069667026)\n",
    "    \n",
    "    nodes.mlab_source.dataset.point_data.scalars = color\n",
    "    print('showing pc')\n",
    "    aa, bb = -95, -40 #np.random.randint(-105, -85), np.random.randint(-55, -35)\n",
    "    print(aa, bb)\n",
    "    mayavi.mlab.view(azimuth=-87, elevation=-40, focalpoint=(0, 0, np.median(velo[:, -1])))\n",
    "    f = mayavi.mlab.gcf()\n",
    "    f.scene.camera.zoom(2.7)\n",
    "\n",
    "    if save:\n",
    "        print(save)\n",
    "        mayavi.mlab.savefig('../inter_images_2/{}.png'.format(i))\n",
    "        mayavi.mlab.close()\n",
    "    elif save_path is not None:\n",
    "        mayavi.mlab.savefig(save_path)\n",
    "        mayavi.mlab.close()\n",
    "    else:\n",
    "        mayavi.mlab.show()\n",
    "\n",
    "def show_pc_lite(velo, ind=1, show=True):\n",
    "    velo=velo.cpu()\n",
    "    # print(velo.shape)\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.scatter(velo[:, 0], velo[:, 1], s=0.7, color='k')\n",
    "    plt.show() \n",
    "\n",
    "\n",
    "def to_attr(args_dict):\n",
    "    class AttrDict(dict):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super(AttrDict, self).__init__(*args, **kwargs)\n",
    "            self.__dict__ = self\n",
    "\n",
    "    return AttrDict(args_dict)\n",
    "\n",
    "\n",
    "def load_model_from_file(path, epoch, model='dis'):\n",
    "    from models import netD, netG, VAE\n",
    "    import json\n",
    "    with open(os.path.join(path, 'args.json'), 'r') as f: \n",
    "        old_args = json.load(f)\n",
    "\n",
    "    old_args = to_attr(old_args)\n",
    "    if 'gen' in model.lower():\n",
    "        try:\n",
    "            z_ = old_args.z_dim\n",
    "            model_ = VAE(old_args)\n",
    "        except:\n",
    "            z_ = 100\n",
    "            model_ = netG(old_args, nz=z_, nc= 3 if old_args.no_polar else 2)\n",
    "    elif 'dis' in model.lower():\n",
    "        model_ = netD(old_args)\n",
    "    else: \n",
    "        raise ValueError('%s is not a valid model name' % model)\n",
    "\n",
    "    model_.load_state_dict(torch.load(os.path.join(path, 'models/%s_%d.pth' % (model, epoch))))\n",
    "    print('model successfully loaded')\n",
    "\n",
    "    return model_, epoch \n",
    "\n",
    "\n",
    "def batch_pairwise_dist(A, B):\n",
    "    # pa, pb are bs x points x 3\n",
    "    r_A = (A * A).sum(dim=2, keepdim=True)\n",
    "    r_B = (B * B).sum(dim=2, keepdim=True)\n",
    "    m = torch.bmm(A, B.permute(0, 2, 1))\n",
    "    D = r_A - 2 * m + r_B.permute(0, 2, 1)\n",
    "    return D\n",
    "\n",
    "def chamfer_quadratic(a,b):\n",
    "    D = batch_pairwise_dist(a,b)\n",
    "    return D.min(dim=-1)[0], D.min(dim=-2)[0]\n",
    "\n",
    "\n",
    "# Utilities for baseline\n",
    "def get_chamfer_dist(get_slow=False):\n",
    "    try:\n",
    "        if get_slow: raise ValueError\n",
    "\n",
    "        import sys\n",
    "        sys.path.insert(0, './nndistance')\n",
    "        from modules.nnd import NNDModule\n",
    "        dist = NNDModule()\n",
    "    except:\n",
    "        dist = chamfer_quadratic\n",
    "\n",
    "    def loss(a, b):\n",
    "        if a.dim() == 4:\n",
    "            if a.size(1) == 2: \n",
    "                a = from_polar(a)\n",
    "\n",
    "            assert a.size(1) == 3\n",
    "            a = a.permute(0, 2, 3, 1).contiguous().reshape(a.size(0), -1, 3)\n",
    "            \n",
    "        if b.dim() == 4:\n",
    "            if b.size(1) == 2: \n",
    "                b = from_polar(b)\n",
    "\n",
    "            assert b.size(1) == 3\n",
    "            b = b.permute(0, 2, 3, 1).contiguous().reshape(b.size(0), -1, 3)\n",
    "\n",
    "        assert a.dim() == b.dim() == 3\n",
    "        if a.size(-1) != 3: \n",
    "            assert a.size(-2) == 3\n",
    "            a = a.transpose(-2, -1).contiguous()\n",
    "        \n",
    "        if b.size(-1) != 3: \n",
    "            assert b.size(-2) == 3\n",
    "            b = a.transpose(-2, -1).contiguous()\n",
    "\n",
    "        dist_a, dist_b = dist(a, b)\n",
    "        return dist_a.sum(dim=-1) + dist_b.sum(dim=-1)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     import pdb; pdb.set_trace()\n",
    "#     # check if both chamfer implementations give the same results\n",
    "#     ch_fast = get_chamfer_dist()\n",
    "#     ch_slow = get_chamfer_dist(get_slow=True)\n",
    "\n",
    "#     for _ in range(10):\n",
    "#         x = torch.cuda.FloatTensor(32, 1000, 3).normal_()\n",
    "#         y = torch.cuda.FloatTensor(32, 1000, 3).normal_()\n",
    "        \n",
    "#         out_fast = ch_fast(x,y)\n",
    "#         out_slow = ch_slow(x,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T22:31:15.110041Z",
     "start_time": "2020-02-06T22:31:07.145447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3056, 16, 1024, 4)\n"
     ]
    }
   ],
   "source": [
    "STATIC_TRAIN_FOLDER_PATH  = '/home/saby/Projects/ati/data/data/datasets/Carla/16beam-Data/small_map/validate_idea/dynamic_24/_segment_out_npy/'\n",
    "static_dataset_train = np.load(os.path.join(STATIC_TRAIN_FOLDER_PATH,\"0.npy\"))\n",
    "print(static_dataset_train.shape)\n",
    "static_dataset_train[:,:,:,:][(static_dataset_train[:,:,:,3]!=0)&(static_dataset_train[:,:,:,3]!=1)]=0\n",
    "# static_dataset_train = preprocess(static_dataset_train,100).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T12:09:09.718209Z",
     "start_time": "2020-02-06T12:09:08.887761Z"
    }
   },
   "outputs": [],
   "source": [
    "validation = np.load(os.path.join(STATIC_TRAIN_FOLDER_PATH,\"0.npy\"))\n",
    "# print(static_dataset_train[12,2,:,:][(static_dataset_train[12,2,:,:]!=1)&(static_dataset_train[12,2,:,:]!=0)])\n",
    "\n",
    "print(static_dataset_train[2,0,:,:][static_dataset_train[2,2,:,:]==1].shape)\n",
    "print(static_dataset_train[2,2,:,:][static_dataset_train[2,2,:,:]==1].shape)\n",
    "\n",
    "\n",
    "# final=np.array(static_dataset_train[10,2:,:])\n",
    "# print(final.sum())\n",
    "\n",
    "validation[:,:,:,:][(validation[:,:,:,3]!=0)&(validation[:,:,:,3]!=1)]=0\n",
    "print(validation[2,:,:,:][validation[2,:,:,3]==1].shape)\n",
    "\n",
    "# final_2=np.array(validation[10,:,:,3])\n",
    "# print(final_2.sum())\n",
    "\n",
    "# print(static_dataset_train[10,2,:,:][(static_dataset_train[10,2,:,:]!=1)&(static_dataset_train[10,2,:,:]!=0)])\n",
    "# print(validation[10,:,:,3])\n",
    "\n",
    "\n",
    "# print(\"pehla:\",set(zip(results[0],results[1])))\n",
    "# validation=np.where(validation[0,:,:,3]==1)\n",
    "# print(\"doosra:\",set(zip(validation[0],validation[1])))\n",
    "\n",
    "\n",
    "\n",
    "array_1=validation\n",
    "array_2=static_dataset_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T12:09:20.960392Z",
     "start_time": "2020-02-06T12:09:20.941617Z"
    }
   },
   "outputs": [],
   "source": [
    "array_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T12:09:27.006078Z",
     "start_time": "2020-02-06T12:09:26.998683Z"
    }
   },
   "outputs": [],
   "source": [
    "array_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T12:43:25.968831Z",
     "start_time": "2020-02-06T12:43:25.960265Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = np.random.choice(array_1.shape[0],1)[0]\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T12:43:26.426126Z",
     "start_time": "2020-02-06T12:43:26.421333Z"
    }
   },
   "outputs": [],
   "source": [
    "before_img = array_1[idx]\n",
    "after_img  = array_2[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T12:43:26.785854Z",
     "start_time": "2020-02-06T12:43:26.781250Z"
    }
   },
   "outputs": [],
   "source": [
    "after_img_3d = from_polar_np(np.array([after_img[:2,:,:]]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T12:43:27.050865Z",
     "start_time": "2020-02-06T12:43:27.044315Z"
    }
   },
   "outputs": [],
   "source": [
    "after_img_4d = np.concatenate((after_img_3d, np.array([after_img[2]])), axis=0).transpose(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T12:43:27.362430Z",
     "start_time": "2020-02-06T12:43:27.355720Z"
    }
   },
   "outputs": [],
   "source": [
    "before_img.shape, after_img.shape, after_img_4d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T12:43:28.665441Z",
     "start_time": "2020-02-06T12:43:28.653989Z"
    }
   },
   "outputs": [],
   "source": [
    "before_pts_arr = before_img.reshape(-1,4)[:,:3]\n",
    "before_color_arr = np.array([before_img.reshape(-1,4)[:,3], np.zeros(16*1024), np.zeros(16*1024)]).T\n",
    "\n",
    "\n",
    "after_pts_arr = after_img_4d.reshape(-1,4)[:,:3]\n",
    "after_color_arr = np.array([after_img_4d.reshape(-1,4)[:,3], np.zeros(16*1024), np.zeros(16*1024)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T12:43:32.194567Z",
     "start_time": "2020-02-06T12:43:32.190351Z"
    }
   },
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "# o3d.visualization.draw_geometries([pcd_patanahi,pcd_dynamic,pcd_static],width=1280, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T12:43:32.589843Z",
     "start_time": "2020-02-06T12:43:32.557531Z"
    }
   },
   "outputs": [],
   "source": [
    "before_pcd = o3d.geometry.PointCloud()\n",
    "before_pcd.points = o3d.utility.Vector3dVector(before_pts_arr)\n",
    "before_pcd.colors = o3d.utility.Vector3dVector(before_color_arr)\n",
    "\n",
    "after_pcd = o3d.geometry.PointCloud()\n",
    "after_pcd.points = o3d.utility.Vector3dVector(after_pts_arr)\n",
    "after_pcd.colors = o3d.utility.Vector3dVector(after_color_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T12:43:33.225702Z",
     "start_time": "2020-02-06T12:43:33.213526Z"
    }
   },
   "outputs": [],
   "source": [
    "def draw_pcd(pcd, where='opn_nb'):\n",
    "    if where is 'opn_nb':\n",
    "        visualizer = o3d.JVisualizer()\n",
    "        visualizer.add_geometry(pcd)\n",
    "        visualizer.show()\n",
    "    elif where is 'opn_view':\n",
    "        o3d.visualization.draw_geometries([pcd], width=1280, height=800)\n",
    "    elif where is 'mat_3d':\n",
    "        plt.figure()\n",
    "        pts = np.asarray(pcd.points)\n",
    "        plt.scatter(pts[:,0], pts[:,1], pts[:,2])\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    elif where is 'mat_2d':\n",
    "        plt.figure()\n",
    "        pts = np.asarray(pcd.points)\n",
    "        plt.scatter(pts[:,0], pts[:,1])\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T12:43:34.153115Z",
     "start_time": "2020-02-06T12:43:33.747116Z"
    }
   },
   "outputs": [],
   "source": [
    "draw_pcd(before_pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T12:43:47.083420Z",
     "start_time": "2020-02-06T12:43:46.673475Z"
    }
   },
   "outputs": [],
   "source": [
    "draw_pcd(after_pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T18:33:23.021142Z",
     "start_time": "2020-02-05T18:33:23.017665Z"
    }
   },
   "outputs": [],
   "source": [
    "a=torch.randn(1,1,10,10)\n",
    "b=torch.randn(1,1,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T18:33:44.472044Z",
     "start_time": "2020-02-05T18:33:44.456143Z"
    }
   },
   "outputs": [],
   "source": [
    "print(a)\n",
    "print(b)\n",
    "pad=1\n",
    "ks=2\n",
    "out_a,indices = F.max_pool2d(a, ks, padding=pad, stride=1,return_indices=True)\n",
    "print(out_a)\n",
    "print(indices)\n",
    "output=retrieve_elements_from_indices(b,indices)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T00:04:19.684261Z",
     "start_time": "2020-02-07T00:04:19.680236Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T01:58:31.905215Z",
     "start_time": "2020-02-07T01:58:31.892032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 1024])\n",
      "tensor([[[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]]])\n",
      "tensor(0.9009, grad_fn=<NllLoss2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input_1 = torch.randn(1,2, 16,1024, requires_grad=True)\n",
    "target = torch.zeros(1,2,16 ,1024,dtype=torch.long)\n",
    "target_2 = torch.zeros(1,2,16 ,1024)\n",
    "print(input_1.squeeze(0).shape)\n",
    "print(target)\n",
    "\n",
    "output = loss(input_1, target[:,0])\n",
    "# output.backward()\n",
    "\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
